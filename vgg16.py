# -*- coding: utf-8 -*-
"""VGG16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PWA_SeaXQtqp-xHfIi4Hqmj_FAanpubW
"""

# Commented out IPython magic to ensure Python compatibility.
import os
from pathlib import Path
import numpy as np
import pandas as pd
from PIL import Image
import cv2
import matplotlib.pyplot as plt
# %matplotlib inline 
import tensorflow as tf

from keras.models import Model
from keras.optimizers import Adam
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from keras.layers import Dense, Dropout, Flatten


from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import shuffle
#from keras.layers import Concatenate

from tensorflow.keras import layers
from tensorflow.keras import Input
from tensorflow.keras.utils import to_categorical

from google.colab import drive
drive.mount('/MyDrive')

df = pd.read_pickle("/MyDrive/MyDrive/DataScience/even_class.pkl")
df.info()

df['label'].value_counts()

encoder = LabelEncoder()
df['label'] = encoder.fit_transform(df.label)
df['label'].value_counts()

#Dispaly an img_data
plt.imshow(df['img_data'].iloc[600])

# Shuffle the DataFrame, and split into train, validation and test sets

shuffled_df = df.sample(frac=1)

data = df.img_data
target = df.label

#train test split
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=.2)

#train validation split
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.2)

print(len(X_train), len(y_train))
print(len(X_val), len(y_val))
print(len(X_test), len(y_test))

X_train = np.stack(X_train, axis=0)
X_val = np.stack(X_val, axis=0)
X_test = np.stack(X_test, axis=0)

y_train = to_categorical(y_train)
y_val = to_categorical(y_val)
y_test = to_categorical(y_test)

#augmentation layer

data_augmentation = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
    layers.experimental.preprocessing.RandomRotation(0.2), 
    #layers.experimental.preprocessing.Rescaling(1./255),
    layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),
    layers.experimental.preprocessing.RandomTranslation(0.3, 0.3)
    ])



# Define the Model

def create_model(n_labels, fine_tune = 0):
    base_model = VGG16(include_top=False, weights='imagenet', input_shape=(360, 360, 3))
    
    if fine_tune > 0:
        for layer in base_model.layers[:-fine_tune]:
            layer.trainable = False
    else:
        for layer in base_model.layers:
            layer.trainable = False
            
    inputs    = Input(shape=(360, 360, 3))
    rescale   = preprocess_input(inputs)
    augmented = data_augmentation(rescale)
    vgg16     = base_model(rescale)
    pooling   = layers.GlobalAveragePooling2D()(vgg16)
    dense1     = Dense(1024, activation='relu')(pooling)
    dropout1   = Dropout(rate=0.2)(dense1)
    dense2     = Dense(512, activation='relu')(dropout1)
    dropout2   = Dropout(0.2)(dense2)
    outputs   = Dense(n_labels, activation='softmax')(dropout2)
    model     = Model(inputs=inputs, outputs=outputs)

    return model

model = create_model(len(df['label'].unique()))
model.summary()

optimizer = Adam(learning_rate=0.001)

# Compiles the model for training.
model.compile(optimizer=optimizer, 
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

history = model.fit(x= X_train, y=y_train,
                    epochs = 10, 
                    validation_data=(X_val, y_val),
                    verbose=1)

# plot the loss
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.show()

# plot the accuracy
plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()





# Generate predictions

from sklearn.metrics import accuracy_score, f1_score

predicted = model.predict(X_test)                        # predicted values or probabilities?
predicted_classes = np.argmax(predicted, axis=-1)        # predicted classes

true_classes = np.argmax(y_test, axis=-1)                # equal to y_test

acc_score = accuracy_score(true_classes, predicted_classes)
f1_score = f1_score(true_classes, predicted_classes, average="weighted")

print(f"Model Accuracy without Fine-Tuning: {round(acc_score * 100, 2)}")
print(f"Model F1 Score without Fine-Tuning: {round(f1_score * 100, 2)}")

#classification report
from sklearn import metrics
print(metrics.classification_report(true_classes, predicted_classes))

import itertools

cnf_matrix = metrics.confusion_matrix(true_classes, predicted_classes)
print(cnf_matrix)
classes = range(0,8)

plt.figure()

plt.imshow(cnf_matrix, interpolation='nearest',cmap='Blues')
plt.title("confusion matrix")
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes)
plt.yticks(tick_marks, classes)

for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):
    plt.text(j, i, cnf_matrix[i, j],
             horizontalalignment="center",
             color="white" if cnf_matrix[i, j] > ( cnf_matrix.max() / 2) else "black")

plt.ylabel('Actual values')
plt.xlabel('Predicted values')
plt.show()

# Fine Tuning

model = create_model(len(df['label'].unique()), 3)
model.summary()

optimizer = Adam(learning_rate=0.0001)

# Compiles the model for training.
model.compile(optimizer=optimizer, 
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

history = model.fit(x= X_train, y=y_train,
                    epochs = 10, 
                    validation_data=(X_val, y_val),
                    verbose=1)

# plot the loss
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.show()

# plot the accuracy
plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score, f1_score

predicted = model.predict(X_test)                        # predicted values or probabilities?
predicted_classes = np.argmax(predicted, axis=-1)        # predicted classes

true_classes = np.argmax(y_test, axis=-1)                # equal to y_test

acc_score = accuracy_score(true_classes, predicted_classes)
f1_score = f1_score(true_classes, predicted_classes, average="weighted")

print(f"Model Accuracy with Fine-Tuning: {round(acc_score * 100, 2)}")
print(f"Model F1 Score with Fine-Tuning: {round(f1_score * 100, 2)}")

#classification report
from sklearn import metrics
print(metrics.classification_report(true_classes, predicted_classes))

import itertools

cnf_matrix = metrics.confusion_matrix(true_classes, predicted_classes)
print(cnf_matrix)
classes = range(0,8)

plt.figure()

plt.imshow(cnf_matrix, interpolation='nearest',cmap='Blues')
plt.title("confusion matrix")
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes)
plt.yticks(tick_marks, classes)

for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):
    plt.text(j, i, cnf_matrix[i, j],
             horizontalalignment="center",
             color="white" if cnf_matrix[i, j] > ( cnf_matrix.max() / 2) else "black")

plt.ylabel('Actual values')
plt.xlabel('Predicted values')
plt.show()

#Save model
from tensorflow.keras.models import save_model
save_model(model, "/MyDrive/MyDrive/DataScience/vgg16_model.h5")

#Displayed Actual values & Predicted values column wise
df_tested = pd.DataFrame(zip(true_classes, predicted_classes),
                  columns=['Actual Value', 'Predicted Value'])

predicted_classes_name = df_tested['Predicted Value'].map( {0:'basophil',
                                                            1: 'neutrophil',
                                                            2: 'monocyte',
                                                            3: 'lymphocyte',
                                                            4: 'platelet',
                                                            5: 'eosinophil',
                                                            6: 'ig',
                                                            7: 'erythroblast'}).astype(str)

df_tested['Predicted Class'] = predicted_classes_name
df_tested